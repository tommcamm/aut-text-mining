{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY891NWn2K-P"
      },
      "source": [
        "# Text Mining - Assignment\n",
        "Due 7th june by midnight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab, downloading all the data')\n",
        "  # download the pre-processed datasets\n",
        "  ## -nc avoid to download the file if already present, -P is the directory where the file will be placed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/students_preprocessed.pkl.gz -P PreProcessed # students pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/under_20s_preprocessed.pkl.gz -P PreProcessed # under 20s pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/females_preprocessed.pkl.gz -P PreProcessed # females pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/males_preprocessed.pkl.gz -P PreProcessed # males pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/over_20s_preprocessed.pkl.gz -P PreProcessed # over 20s pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/everyone_preprocessed.pkl.gz -P PreProcessed # Everyone pre-processed\n",
        "\n",
        "  # Download the test data - two files\n",
        "  !wget -nc https://raw.githubusercontent.com/tommcamm/aut-text-mining/main/Assignment/code/TestDir/23676.male.33.Technology.Scorpio.xml -P TestDir\n",
        "  !wget -nc https://raw.githubusercontent.com/tommcamm/aut-text-mining/main/Assignment/code/TestDir/5114.male.25.indUnk.Scorpio.xml -P TestDir\n",
        "else:\n",
        "  print('Not running on CoLab, skipping download')\n",
        "  # For this step I assume the data is already there\n",
        "  directory_path = './Assignment2BlogData/blogs'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABbRHDBa7Ui2",
        "outputId": "aea71cf2-f8d1-4f26-f33e-6e1b08df3791"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab, downloading all the data\n",
            "File ‘PreProcessed/students_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/under_20s_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/females_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/males_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/over_20s_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/everyone_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘TestDir/23676.male.33.Technology.Scorpio.xml’ already there; not retrieving.\n",
            "\n",
            "File ‘TestDir/5114.male.25.indUnk.Scorpio.xml’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnqgXm7l8lbR"
      },
      "source": [
        "## Data cleaning\n",
        "The following steps will be applied to the dataset to ensure it is cleaned.\n",
        "1. Remove Non-ASCII Characters: Ensures text is ASCII encoded.\n",
        "2. Remove Punctuation: Removes any punctuation marks.\n",
        "3. Lowercase Conversion: Converts all text to lowercase.\n",
        "4. Remove Stopwords: Removes common stopwords that do not contribute to the meaning of the text.\n",
        "5. Tokenization: Splits text into individual words.\n",
        "6. Lemmatization: Reduces words to their base or root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOnkMyOK86bB",
        "outputId": "15d938a2-a42a-40d6-a34a-e5be3f50c3cd",
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:07.997432Z",
          "start_time": "2024-06-01T07:18:06.026130Z"
        }
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "import chardet\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# this command must be run before: python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#spacy.require_gpu() # Ensure is using GPU\n",
        "#spacy.require_cpu()\n",
        "\n",
        "# Text pre-processing pipeline\n",
        "def preprocess_text(text):\n",
        "    # 1. We remove all XML tags from the document (along with the date)\n",
        "    text = re.sub(r'<date>.*?</date>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'<[^>]+>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'urlLink', '', text, flags=re.DOTALL) # Remove links\n",
        "\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii') # Remove non ASCII characters\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation\n",
        "\n",
        "\n",
        "    text = text.lower() # Lowercasing to make it case-insensitive\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Map POS tag to first character lemmatize() accepts\n",
        "    def get_wordnet_pos(tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ  # adjective\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB  # verb\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN  # noun\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV  # adverb\n",
        "        else:\n",
        "            return wordnet.NOUN  # default to noun\n",
        "\n",
        "    cleaned_tokens = [\n",
        "        (lemmatizer.lemmatize(token, get_wordnet_pos(tag)), tag)\n",
        "        for token, tag in tagged_tokens\n",
        "        if token not in stopwords.words('english')\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Pre-Process pipeline using spacy for GPU\n",
        "def preprocess_text_spacy(text):\n",
        "    # 1. Remove all XML tags from the document (along with the date)\n",
        "    text = re.sub(r'<date>.*?</date>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'<[^>]+>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'urlLink', '', text, flags=re.DOTALL) # Remove links\n",
        "\n",
        "    # Convert to ASCII and lowercasing to make it case-insensitive\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = text.lower()\n",
        "\n",
        "    # Process the text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    cleaned_tokens = [\n",
        "        (token.lemma_, token.pos_)\n",
        "        for token in doc\n",
        "        if not token.is_stop and token.is_alpha\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "def process_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "            result = chardet.detect(raw_data)\n",
        "            encoding = result['encoding']\n",
        "            text = raw_data.decode(encoding)\n",
        "            cleaned_text = preprocess_text_spacy(text)\n",
        "            return cleaned_text, None\n",
        "    except Exception as e:\n",
        "        return None, (filepath, str(e))\n",
        "\n",
        "def extract_and_preprocess_text_from_directory(directory_path, filter_func=None):\n",
        "    text_data = []\n",
        "    failed_files = []\n",
        "    filepaths = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\n",
        "\n",
        "    if filter_func:\n",
        "        filepaths = [fp for fp in filepaths if filter_func(fp)]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(process_file, filepath): filepath for filepath in filepaths}\n",
        "        with tqdm(total=len(filepaths), desc=\"Processing files\") as pbar:\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                cleaned_text, error = future.result()\n",
        "                if cleaned_text:\n",
        "                    text_data.append(cleaned_text)\n",
        "                else:\n",
        "                    failed_files.append(error)\n",
        "                pbar.update(1)\n",
        "    return text_data, failed_files\n",
        "\n",
        "# Helper functions\n",
        "def get_tokens_without_pos(doc):\n",
        "    \"\"\"\n",
        "    Extracts tokens without POS tags from the document.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :return: List of tokens\n",
        "    \"\"\"\n",
        "    return [token for token, _ in doc]\n",
        "\n",
        "def get_text_from_tokens(doc):\n",
        "    \"\"\"\n",
        "    Constructs a string from tokens without POS tags.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :return: String of concatenated tokens\n",
        "    \"\"\"\n",
        "    tokens_only = get_tokens_without_pos(doc)\n",
        "    return ' '.join(tokens_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.906994Z",
          "start_time": "2024-06-01T07:18:07.997488Z"
        },
        "id": "Hlyula1x6kMh",
        "outputId": "bc7f5e34-52cb-4bdd-fb53-a957d37be26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Test of the pre-processor on one file\n",
        "test_dir = 'TestDir'\n",
        "text_data_test, failed_data_test = extract_and_preprocess_text_from_directory(test_dir)\n",
        "\n",
        "print('\\nTEST RESULTS (23676, 5114)')\n",
        "for doc in text_data_test:\n",
        "    print(\"-> \", get_text_from_tokens(doc[:20]))\n",
        "\n",
        "print('WITH POS TAGS')\n",
        "for doc in text_data_test:\n",
        "    print(\"-> \", doc[:10])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST RESULTS (23676, 5114)\n",
            "->  hello run finally end smooth win static congrat gil box quick sell ni wednesday hold xping goodness pm edt usual\n",
            "->  slashdot raise lot interesting thought banner ad idea let user control ad delivery allow user comment ad merchant cool frontline\n",
            "WITH POS TAGS\n",
            "->  [('hello', 'INTJ'), ('run', 'NOUN'), ('finally', 'ADV'), ('end', 'VERB'), ('smooth', 'ADJ'), ('win', 'NOUN'), ('static', 'NOUN'), ('congrat', 'NOUN'), ('gil', 'NOUN'), ('box', 'NOUN')]\n",
            "->  [('slashdot', 'NOUN'), ('raise', 'VERB'), ('lot', 'NOUN'), ('interesting', 'ADJ'), ('thought', 'NOUN'), ('banner', 'NOUN'), ('ad', 'NOUN'), ('idea', 'NOUN'), ('let', 'VERB'), ('user', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q2cfEiZ9_ZI",
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.912999Z",
          "start_time": "2024-06-01T07:18:08.907563Z"
        }
      },
      "source": [
        "# Filters for the pre-processor\n",
        "\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "def filter_everyone(filepath):\n",
        "    return True\n",
        "\n",
        "def filter_student(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.Student.' in filename\n",
        "\n",
        "def filter_female(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.female.' in filename\n",
        "\n",
        "def filter_male(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.male.' in filename\n",
        "\n",
        "def filter_age_over_20(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        age = int(filename.split('.')[2])\n",
        "        return age > 20\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def filter_age_under_20(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        age = int(filename.split('.')[2])\n",
        "        return age <= 20\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "#  Helper function to work with the pre-processed data\n",
        "#  (they would be much bigger)\n",
        "def save_compressed_pickle(data, filename):\n",
        "    with gzip.open(filename, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def load_compressed_pickle(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.930244Z",
          "start_time": "2024-06-01T07:18:08.914004Z"
        },
        "id": "xq3E0US76kMh"
      },
      "cell_type": "code",
      "source": [
        "# Pre-Processing block, everything commented to avoid accidental run\n",
        "\n",
        "# Process \"Students\"\n",
        "#text_data_students, failed_files_students = extract_and_preprocess_text_from_directory(directory_path, filter_student)\n",
        "#save_compressed_pickle(text_data_students, 'students_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"under 20s\"\n",
        "#text_data_under_20s, failed_files_under_20s = extract_and_preprocess_text_from_directory(directory_path, filter_age_under_20)\n",
        "#save_compressed_pickle(text_data_under_20s, 'under_20s_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"males\"\n",
        "#text_data_males, failed_files_males = extract_and_preprocess_text_from_directory(directory_path, filter_male)\n",
        "#save_compressed_pickle(text_data_males, 'males_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"females\"\n",
        "#text_data_females, failed_files_females = extract_and_preprocess_text_from_directory(directory_path, filter_female)\n",
        "#save_compressed_pickle(text_data_females, 'females_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"Over 20s\"\n",
        "#text_data_over_20s, failed_files_over_20s = extract_and_preprocess_text_from_directory(directory_path, filter_age_over_20)\n",
        "#save_compressed_pickle(text_data_over_20s, 'over_20s_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"Everyone\" - If RAM >= 30GB && CPU_Cores > 10 --> approx 2h (colab not recommended (cpu_cores = 2))\n",
        "#text_data_everyone, failed_files_everyone = extract_and_preprocess_text_from_directory(directory_path, filter_everyone)\n",
        "#save_compressed_pickle(text_data_everyone, 'everyone_preprocessed.pkl.gz')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:23:46.616176Z",
          "start_time": "2024-06-01T07:23:41.952413Z"
        },
        "id": "Be_-Dhjr6kMh"
      },
      "cell_type": "code",
      "source": [
        "# Load of all pre-processed files\n",
        "# WARNING: Uses lot of RAM, High-RAM or only one file at a time recommended.\n",
        "students_data = load_compressed_pickle('PreProcessed/students_preprocessed.pkl.gz')\n",
        "under_20s_data = load_compressed_pickle('PreProcessed/under_20s_preprocessed.pkl.gz')\n",
        "females_data = load_compressed_pickle('PreProcessed/females_preprocessed.pkl.gz')\n",
        "males_data = load_compressed_pickle('PreProcessed/males_preprocessed.pkl.gz')\n",
        "over_20s_data = load_compressed_pickle('PreProcessed/over_20s_preprocessed.pkl.gz')\n",
        "everyone_data = load_compressed_pickle('PreProcessed/everyone_preprocessed.pkl.gz')\n",
        "\n",
        "# Dict containin\n",
        "all_preprocessed = {\n",
        "    \"students\": students_data,\n",
        "    \"under_20s\": under_20s_data,\n",
        "    \"females\": females_data,\n",
        "    \"males\": males_data,\n",
        "    \"over_20s\": over_20s_data,\n",
        "    \"everyone\": everyone_data\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modeling by counting all types of nouns\n",
        "\n",
        "The first strategy to extract the 2 most common topics will be by the most prevalent nouns.\n",
        "During the Pre-Processing we assign to each token a POS tag, the TAG that we use is [Universal POS tag](https://universaldependencies.org/u/pos/).\n",
        "\n",
        "These tags mark the core part-of-speech categories, by filtering for the `NOUN` tag we capture all nouns types."
      ],
      "metadata": {
        "id": "oAkZJePBXXok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_nouns(doc):\n",
        "    \"\"\"\n",
        "    Extracts nouns from a document.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :return: List of nouns\n",
        "    \"\"\"\n",
        "    return [word for word, pos in doc if pos.startswith('NOUN')]\n",
        "\n",
        "noun_counters = {}\n",
        "\n",
        "# noun counting pipeline\n",
        "for key, data in all_preprocessed.items():\n",
        "    nouns = [get_nouns(doc) for doc in data]\n",
        "    all_nouns = [noun for sublist in nouns for noun in sublist]\n",
        "    noun_counter = Counter(all_nouns)\n",
        "    noun_counters[key] = noun_counter\n",
        "\n",
        "    # Print number of nouns and the most common nouns\n",
        "    print(f\"[{key.capitalize()}] Number of nouns: {len(all_nouns)}\")\n",
        "    most_common_nouns = noun_counter.most_common(10)\n",
        "    print(f\"[{key.capitalize()}] Most Prevalent Topics (Nouns):\", most_common_nouns)\n",
        "\n",
        "# Store the result\n",
        "save_compressed_pickle(noun_counters, 'noun_counters.pkl.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXxEn3_2XpnT",
        "outputId": "56316393-cdd5-4d8d-8d99-963fbb448138"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Students] Number of nouns: 4892782\n",
            "[Students] Most Prevalent Topics (Nouns): [('time', 90879), ('day', 74564), ('thing', 72508), ('people', 61680), ('today', 52208), ('friend', 42080), ('life', 41866), ('way', 39479), ('school', 35590), ('year', 34219)]\n",
            "[Under_20s] Number of nouns: 7130208\n",
            "[Under_20s] Most Prevalent Topics (Nouns): [('time', 131476), ('day', 112528), ('thing', 108051), ('people', 92919), ('today', 82407), ('friend', 63230), ('life', 60220), ('way', 57889), ('school', 56089), ('guy', 47444)]\n",
            "[Females] Number of nouns: 11176372\n",
            "[Females] Most Prevalent Topics (Nouns): [('time', 207850), ('day', 167650), ('thing', 161760), ('people', 131221), ('today', 100886), ('life', 95444), ('friend', 93739), ('way', 92701), ('year', 85481), ('night', 84913)]\n",
            "[Males] Number of nouns: 11360881\n",
            "[Males] Most Prevalent Topics (Nouns): [('time', 189011), ('day', 140226), ('people', 134905), ('thing', 133016), ('way', 85280), ('year', 85054), ('today', 82984), ('life', 80592), ('friend', 67221), ('night', 62148)]\n",
            "[Over_20s] Number of nouns: 15407045\n",
            "[Over_20s] Most Prevalent Topics (Nouns): [('time', 265385), ('day', 195348), ('thing', 186725), ('people', 173207), ('year', 123164), ('way', 120092), ('life', 115816), ('today', 101463), ('night', 101343), ('friend', 97730)]\n",
            "[Everyone] Number of nouns: 22537253\n",
            "[Everyone] Most Prevalent Topics (Nouns): [('time', 396861), ('day', 307876), ('thing', 294776), ('people', 266126), ('today', 183870), ('way', 177981), ('life', 176036), ('year', 170535), ('friend', 160960), ('night', 147061)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noun counting - Part (2) - Clause extraction\n",
        "\n",
        "# load noun counter\n",
        "noun_counters = load_compressed_pickle('noun_counters.pkl.gz')\n",
        "\n",
        "# Function to extract clauses containing the top topics\n",
        "def extract_clauses(doc, topics, max_clause_len=50):\n",
        "    \"\"\"\n",
        "    Extracts clauses containing the specified topics from a document.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :param topics: List of top topics (nouns)\n",
        "    :param max_clause_len: Maximum length of a clause\n",
        "    :return: List of clauses containing the topics\n",
        "    \"\"\"\n",
        "    clauses = []\n",
        "    current_clause = []\n",
        "\n",
        "    for token, pos in doc:\n",
        "        current_clause.append(token)\n",
        "        if len(current_clause) >= max_clause_len or pos.startswith('VERB'):\n",
        "            if any(topic in current_clause for topic in topics):\n",
        "                clauses.append(' '.join(current_clause))\n",
        "            current_clause = []\n",
        "\n",
        "    # Check last clause if it contains any topics\n",
        "    if any(topic in current_clause for topic in topics):\n",
        "        clauses.append(' '.join(current_clause))\n",
        "\n",
        "    return clauses\n",
        "\n",
        "# Iterate through each demographic to extract clauses with top topics\n",
        "max_clauses_to_print = 5\n",
        "\n",
        "for demographic, data in all_preprocessed.items():\n",
        "    top_topics = [noun for noun, count in noun_counters[demographic].most_common(2)] # Get the two top topics\n",
        "\n",
        "    clauses_with_topics = [extract_clauses(doc, top_topics) for doc in data]\n",
        "    all_clauses_with_topics = [clause for sublist in clauses_with_topics for clause in sublist]\n",
        "\n",
        "    # Extraction\n",
        "    for topic in top_topics:\n",
        "        print(f\"Clauses containing the topic '{topic}' for {demographic}:\")\n",
        "        count = 0\n",
        "        for clause in all_clauses_with_topics:\n",
        "            if topic in clause:\n",
        "                print(f\"- {clause}\")\n",
        "                count += 1\n",
        "            if count >= max_clauses_to_print:\n",
        "                break\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcuCb1acY8jz",
        "outputId": "05a6e846-0155-4f68-dd4d-a90167bbb302"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clauses containing the topic 'time' for students:\n",
            "- precious time beautiful art theatre quit\n",
            "- time work\n",
            "- long time page get\n",
            "- time wxii say\n",
            "- work chemistry paper time watch\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for students:\n",
            "- jeep inherently house arrest oh lovely day outside dog walk bike ride read\n",
            "- meal day go\n",
            "- physical exertion long strenuous day err stuff kill\n",
            "- computer game day collect\n",
            "- violent weapon customary gift valentine day complain\n",
            "\n",
            "\n",
            "Clauses containing the topic 'time' for under_20s:\n",
            "- precious time beautiful art theatre quit\n",
            "- time work\n",
            "- long time page get\n",
            "- time wxii say\n",
            "- work chemistry paper time watch\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for under_20s:\n",
            "- jeep inherently house arrest oh lovely day outside dog walk bike ride read\n",
            "- meal day go\n",
            "- physical exertion long strenuous day err stuff kill\n",
            "- computer game day collect\n",
            "- violent weapon customary gift valentine day complain\n",
            "\n",
            "\n",
            "Clauses containing the topic 'time' for females:\n",
            "- good time lot name past funny barely run\n",
            "- like time want\n",
            "- time child need\n",
            "- woman year time work work\n",
            "- lot mom lot time child care space\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for females:\n",
            "- la day tomorrow release\n",
            "- past weekend good independence day sure well raise\n",
            "- great deal funny thing important high school long glad tired week glad day snooze\n",
            "- past couple day assure\n",
            "- day shatter\n",
            "\n",
            "\n",
            "Clauses containing the topic 'time' for males:\n",
            "- ground information wise short time atom introduce\n",
            "- time atomenable\n",
            "- time use\n",
            "- short period time live webcast spotty good outage occur\n",
            "- luckymojo hoodoo rootwork hour d evening pm eastern time participant hoodoo rootwork course come\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for males:\n",
            "- beach day noah good friend william picture take\n",
            "- groennestrand western jutland vacation june new year day fantastic experience noah white stuff ground main theme son adoption south africa maybe adoption south africa generel noahs mother henriette proud father peter try\n",
            "- hit day july statistic august website redesign\n",
            "- positive vibe web station total listening hour day country tune\n",
            "- day hundred people share\n",
            "\n",
            "\n",
            "Clauses containing the topic 'time' for over_20s:\n",
            "- good time lot name past funny barely run\n",
            "- like time want\n",
            "- time child need\n",
            "- woman year time work work\n",
            "- lot mom lot time child care space\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for over_20s:\n",
            "- la day tomorrow release\n",
            "- past weekend good independence day sure well raise\n",
            "- great deal funny thing important high school long glad tired week glad day snooze\n",
            "- hit day july statistic august website redesign\n",
            "- positive vibe web station total listening hour day country tune\n",
            "\n",
            "\n",
            "Clauses containing the topic 'time' for everyone:\n",
            "- good time lot name past funny barely run\n",
            "- like time want\n",
            "- time child need\n",
            "- woman year time work work\n",
            "- lot mom lot time child care space\n",
            "\n",
            "\n",
            "Clauses containing the topic 'day' for everyone:\n",
            "- la day tomorrow release\n",
            "- past weekend good independence day sure well raise\n",
            "- great deal funny thing important high school long glad tired week glad day snooze\n",
            "- hit day july statistic august website redesign\n",
            "- positive vibe web station total listening hour day country tune\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SDU3M6UI6kMi"
      },
      "cell_type": "markdown",
      "source": [
        "## Topic modeling with Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:46:51.480526Z",
          "start_time": "2024-06-01T07:46:10.434531Z"
        },
        "id": "NrJNMIUJ6kMi",
        "outputId": "c804d9e1-0d5e-4d44-d5c9-ee47ceaad6d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Topic modeling with LDA - Students\n",
        "students_tokens = [get_tokens_without_pos(doc) for doc in students_data]\n",
        "\n",
        "# Dictionary representation of all docs\n",
        "id2word = corpora.Dictionary(students_tokens)\n",
        "\n",
        "# Corpus representation of all docs\n",
        "corpus = [id2word.doc2bow(text) for text in students_tokens]\n",
        "\n",
        "lda_model = gensim.models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=2,\n",
        "    random_state=100,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "print(lda_model.print_topics())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.014*\"like\" + 0.012*\"go\" + 0.009*\"get\" + 0.009*\"know\" + 0.008*\"think\" + 0.008*\"good\" + 0.007*\"time\" + 0.006*\"want\" + 0.006*\"day\" + 0.006*\"thing\"'), (1, '0.004*\"n\" + 0.004*\"time\" + 0.004*\"war\" + 0.003*\"haha\" + 0.003*\"u\" + 0.003*\"bush\" + 0.003*\"man\" + 0.002*\"den\" + 0.002*\"study\" + 0.002*\"e\"')]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T08:06:49.721724Z",
          "start_time": "2024-06-01T07:53:33.401164Z"
        },
        "id": "VOHDSeEG6kMi",
        "outputId": "0016a508-8bfe-4869-d54b-62c7d166338b"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Increase the max_length limit of SpaCy\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, max_chunk_size=1000000):\n",
        "    return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "# Extract tokens from pre-processed students_data\n",
        "def extract_tokens(documents):\n",
        "    return [[token for token, _ in doc] for doc in documents]\n",
        "\n",
        "documents = extract_tokens(students_data)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "id2word = corpora.Dictionary(documents)\n",
        "corpus = [id2word.doc2bow(text) for text in documents]\n",
        "\n",
        "# Build the LDA model using multicore processing\n",
        "lda_model = gensim.models.LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=2,\n",
        "    random_state=100,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Get the top terms for each topic\n",
        "top_terms = lda_model.show_topics(num_words=5)\n",
        "topics = [term for topic in top_terms for term, _ in lda_model.show_topic(topic[0], topn=5)]\n",
        "print(\"Top Terms:\", topics)\n",
        "\n",
        "def extract_sentences_with_topics(text, topics):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents if any(topic in sent.text for topic in topics)]\n",
        "    return sentences\n",
        "\n",
        "# Combine all documents into a single text for sentence extraction\n",
        "combined_text = \" \".join([\" \".join(doc) for doc in documents])\n",
        "\n",
        "# Split the combined text into smaller chunks\n",
        "text_chunks = split_text_into_chunks(combined_text)\n",
        "\n",
        "# Extract sentences with topics from each chunk\n",
        "all_sentences_with_topics = []\n",
        "for chunk in text_chunks:\n",
        "    sentences_with_topics = extract_sentences_with_topics(chunk, topics)\n",
        "    all_sentences_with_topics.extend(sentences_with_topics)\n",
        "\n",
        "print(\"Total sentences with topics:\", len(all_sentences_with_topics))\n",
        "for sentence in all_sentences_with_topics:\n",
        "    print(sentence)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Terms: ['like', 'go', 'love', 'know', 'get', 'like', 'go', 'think', 'time', 'get']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}