{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY891NWn2K-P"
      },
      "source": [
        "# Text Mining - Assignment\n",
        "Due 7th june by midnight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab, downloading all the data')\n",
        "  # download the pre-processed datasets\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/students_preprocessed.pkl.gz -P PreProcessed # students pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/under_20s_preprocessed.pkl.gz -P PreProcessed # under 20s pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/females_preprocessed.pkl.gz -P PreProcessed # females pre-processed\n",
        "  !wget -nc https://github.com/tommcamm/aut-text-mining/raw/main/Assignment/code/PreProcessed/males_preprocessed.pkl.gz -P PreProcessed # males pre-processed\n",
        "\n",
        "  # Download the test data - two files\n",
        "  !wget -nc https://raw.githubusercontent.com/tommcamm/aut-text-mining/main/Assignment/code/TestDir/23676.male.33.Technology.Scorpio.xml -P TestDir\n",
        "  !wget -nc https://raw.githubusercontent.com/tommcamm/aut-text-mining/main/Assignment/code/TestDir/5114.male.25.indUnk.Scorpio.xml -P TestDir\n",
        "else:\n",
        "  print('Not running on CoLab, skipping download')\n",
        "  # For this step the data is already there"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABbRHDBa7Ui2",
        "outputId": "28b28fa1-362e-4e05-f915-0789463e9421"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n",
            "File ‘PreProcessed/students_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/under_20s_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/females_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘PreProcessed/males_preprocessed.pkl.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘TestDir/23676.male.33.Technology.Scorpio.xml’ already there; not retrieving.\n",
            "\n",
            "File ‘TestDir/5114.male.25.indUnk.Scorpio.xml’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZwnL04M3d0Y",
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:06.025123Z",
          "start_time": "2024-06-01T07:18:06.018615Z"
        }
      },
      "source": [
        "# Local environment setup\n",
        "# Setup 2 - Unzip and load all required content\n",
        "directory_path = './Assignment2BlogData/blogs'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnqgXm7l8lbR"
      },
      "source": [
        "## Data cleaning\n",
        "The following steps will be applied to the dataset to ensure it is cleaned.\n",
        "1. Remove Non-ASCII Characters: Ensures text is ASCII encoded.\n",
        "2. Remove Punctuation: Removes any punctuation marks.\n",
        "3. Lowercase Conversion: Converts all text to lowercase.\n",
        "4. Remove Stopwords: Removes common stopwords that do not contribute to the meaning of the text.\n",
        "5. Tokenization: Splits text into individual words.\n",
        "6. Lemmatization: Reduces words to their base or root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOnkMyOK86bB",
        "outputId": "a7e7b1a9-fe25-450f-bf6c-2754d97d12a7",
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:07.997432Z",
          "start_time": "2024-06-01T07:18:06.026130Z"
        }
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "import chardet\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# this command must be run before: python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#spacy.require_gpu() # Ensure is using GPU\n",
        "#spacy.require_cpu()\n",
        "\n",
        "# Text pre-processing pipeline\n",
        "def preprocess_text(text):\n",
        "    # 1. We remove all XML tags from the document (along with the date)\n",
        "    text = re.sub(r'<date>.*?</date>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'<[^>]+>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'urlLink', '', text, flags=re.DOTALL) # Remove links\n",
        "\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii') # Remove non ASCII characters\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text) # Remove punctuation\n",
        "\n",
        "\n",
        "    text = text.lower() # Lowercasing to make it case-insensitive\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Map POS tag to first character lemmatize() accepts\n",
        "    def get_wordnet_pos(tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ  # adjective\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB  # verb\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN  # noun\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV  # adverb\n",
        "        else:\n",
        "            return wordnet.NOUN  # default to noun\n",
        "\n",
        "    cleaned_tokens = [\n",
        "        (lemmatizer.lemmatize(token, get_wordnet_pos(tag)), tag)\n",
        "        for token, tag in tagged_tokens\n",
        "        if token not in stopwords.words('english')\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Pre-Process pipeline using spacy for GPU\n",
        "def preprocess_text_spacy(text):\n",
        "    # 1. Remove all XML tags from the document (along with the date)\n",
        "    text = re.sub(r'<date>.*?</date>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'<[^>]+>', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'urlLink', '', text, flags=re.DOTALL) # Remove links\n",
        "\n",
        "    # Convert to ASCII and lowercasing to make it case-insensitive\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    text = text.lower()\n",
        "\n",
        "    # Process the text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    cleaned_tokens = [\n",
        "        (token.lemma_, token.pos_)\n",
        "        for token in doc\n",
        "        if not token.is_stop and token.is_alpha\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "def process_file(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            raw_data = f.read()\n",
        "            result = chardet.detect(raw_data)\n",
        "            encoding = result['encoding']\n",
        "            text = raw_data.decode(encoding)\n",
        "            cleaned_text = preprocess_text_spacy(text)\n",
        "            return cleaned_text, None\n",
        "    except Exception as e:\n",
        "        return None, (filepath, str(e))\n",
        "\n",
        "def extract_and_preprocess_text_from_directory(directory_path, filter_func=None):\n",
        "    text_data = []\n",
        "    failed_files = []\n",
        "    filepaths = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\n",
        "\n",
        "    if filter_func:\n",
        "        filepaths = [fp for fp in filepaths if filter_func(fp)]\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(process_file, filepath): filepath for filepath in filepaths}\n",
        "        with tqdm(total=len(filepaths), desc=\"Processing files\") as pbar:\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                cleaned_text, error = future.result()\n",
        "                if cleaned_text:\n",
        "                    text_data.append(cleaned_text)\n",
        "                else:\n",
        "                    failed_files.append(error)\n",
        "                pbar.update(1)\n",
        "    return text_data, failed_files\n",
        "\n",
        "# Helper functions\n",
        "def get_tokens_without_pos(doc):\n",
        "    \"\"\"\n",
        "    Extracts tokens without POS tags from the document.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :return: List of tokens\n",
        "    \"\"\"\n",
        "    return [token for token, _ in doc]\n",
        "\n",
        "def get_text_from_tokens(doc):\n",
        "    \"\"\"\n",
        "    Constructs a string from tokens without POS tags.\n",
        "\n",
        "    :param doc: List of tuples (token, pos_tag)\n",
        "    :return: String of concatenated tokens\n",
        "    \"\"\"\n",
        "    tokens_only = get_tokens_without_pos(doc)\n",
        "    return ' '.join(tokens_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.906994Z",
          "start_time": "2024-06-01T07:18:07.997488Z"
        },
        "id": "Hlyula1x6kMh",
        "outputId": "cd998a51-e172-4382-e187-34c786026355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Test of the pre-processor on one file\n",
        "test_dir = 'TestDir'\n",
        "text_data_test, failed_data_test = extract_and_preprocess_text_from_directory(test_dir)\n",
        "\n",
        "for doc in text_data_test:\n",
        "    print(get_text_from_tokens(doc))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aut text mining assignment code tommcamm aut text mining github skip content start accessibility keyboard shortcuts navigation menu toggle navigation sign product action automate workflow package host manage package security find fix vulnerability codespace instant dev environment copilot write well code ai code review manage code change issue plan track work discussion collaborate outside code explore feature documentation github skill blog solution enterprise team startup education solution ci cd amp automation devop devsecop resource learn pathway white paper ebook webinar customer story partner open source github sponsor fund open source developer readme project github community article repository topic trend collection enterprise enterprise platform ai powered developer platform available add ons advanced security enterprise grade security feature copilot enterprise enterprise grade ai feature premium support enterprise grade support pricing search jump search code repository user issue pull request search clear search syntax tip provide feedback read piece feedback input seriously include email address contact cancel submit feedback save search use save search filter result quickly query available qualifier documentation cancel create save search sign sign sign tab window reload refresh session sign tab window reload refresh session switch account tab window reload refresh session dismiss alert message tommcamm aut text mining public notification sign change notification setting fork star code issue pull request action project security insight additional navigation option code issue pull request action project security insight code code code code code code code code ieee text code hello run finally end smooth win static congrat gil box quick sell wednesday hold xping goodness pm edt usual thursday friday hold genkai madness head mold saturday conclude genkai explosure madness bomb sunday run hunt key genkai thank shari deft handling get genkai alliance friday saturday genkai experience mildly horrible instead excruciatingly horrible friday start alliance people eventually grow shrink night go manage obtain need exoray mold mere time read blame dynastey aggro nearly nest case time saturday turn trek eldeime dyn manage aggro half mob glacier personal small group night able dispatch come mere hour reward papyrus death level shari bad outing genkai item probably post hopefully check newly create forum arrr p xp fun night shorty whm rest swashbuckler enjoy individual pursuit generally sharkky run jungle yhoator try find specific word actually find search party dispatch dynastey spend time fishing realize moon favor try luck cooking dog dog need tum shari go level lvl arm stack mithkabob toothpick intrepid taru brave wild sarutabaruta realize wonder count beg o reset skellington run maze shakrami search new place farm instead find lowbie die horrible death hand link stonega require license operate tomorrow genkai madness hello primary user device go lay schedule day let know problem comment find game etc soon messageboard forum stay tune courtesy wednesday pm jeuno xp thursday friday pm jeuno genkai hunt crawler nest eldieme necropolis garlaige saturday pm jeuno genkai hunt sunday monday pm jeuno xp tuesday tuesday july pm static run follow propose static lineup shari blm whm shortcake whm blm sakari brd whm skellington pld war dynastey drk war sharkky thf nin roundabout level jeuno ready play pm happen minor confusion death pick xp night woot heh ok static run friday night pm edt need blogger account post message email lot spam think good subject line post desire static run conflict suggest run post comment thursday run shari thank tonight tuesday head come pm pdt fall congratulation dynastey finish drk weekend dynastey jobom skellington hand kick ass vaa huja erudite shari call supermeganukemasu reinforcement dyn die time second time breeze end victorious congrats let know waterga ii bitch spread word congratulation successful bcnm run member time meeting static party monday july pm edt issue post shari man baby post merely introduce blog scheduling information ffxi lakshmi server swashbuckler linkshell static party shari skellington purpose act like pirate leveling generally have good time bcnm party schedule follow thank gang return regularly schedule program xping night tue thur week exception makeup genkai run pre empt xping say aut text mining network dismiss notice tommcamm aut text mining assignment code sign propose sign propose text aut text mining assignment code dismiss notice aut text mining release tommcamm aut text mining assignment code github create cloning archive repository create repository github citation aut text mining text mining assignment code tommcamm aut text cdn worker find file cdn worker find file footer copy footer navigation term privacy security status doc contact manage cookie share personal information not perform action time\n",
            "aut text mining assignment code tommcamm aut text mining github skip content start accessibility keyboard shortcuts navigation menu toggle navigation sign product action automate workflow package host manage package security find fix vulnerability codespace instant dev environment copilot write well code ai code review manage code change issue plan track work discussion collaborate outside code explore feature documentation github skill blog solution enterprise team startup education solution ci cd amp automation devop devsecop resource learn pathway white paper ebook webinar customer story partner open source github sponsor fund open source developer readme project github community article repository topic trend collection enterprise enterprise platform ai powered developer platform available add ons advanced security enterprise grade security feature copilot enterprise enterprise grade ai feature premium support enterprise grade support pricing search jump search code repository user issue pull request search clear search syntax tip provide feedback read piece feedback input seriously include email address contact cancel submit feedback save search use save search filter result quickly query available qualifier documentation cancel create save search sign sign sign tab window reload refresh session sign tab window reload refresh session switch account tab window reload refresh session dismiss alert message tommcamm aut text mining public notification sign change notification setting fork star code issue pull request action project security insight additional navigation option code issue pull request action project security insight code code code code code code code code ieee text code ok idea stupid idea think overwhelming desire create website work listen hot snake automatic midnight right pretty good kinda obscure fuk goddamn html code right hot snake second rotation misanthropy irony heavy discussion today shit code right fuk fuk hot water music work dammit work supposse ia ok comprise stupid html wear fade black navy blue bar t shirt brown belt etch leather white tennis show moustache play softball old feeble liver spot scatter face notice mouth gape open register string drool stretch tongue rest tooth set tooth hunch forward grip steering wheel tightly wait patiently intersection sure turn happen seven month europe brother uk town bring hearty pile postcard include capuchin cemetary rome capuchin monk take bone dead decorate monastery remain monk adorn monastery death wield scithe hour glass wing human bone seriously creepy shit totally want europe spooky gothic tour europe instead waste time school maybe head pear season cody max apply mos def word pretty demand raise today crakked ha ha dad mention dave barry column sunday bad dave barry suks incident describe dec article santa rosa cal press democrat write mike geniella send alert reader state sergio gutierrez truck driver sea products inc drive tractor trailer highway slam bear cross road gutierrez throw truck overturn result accord story bury avalanche frozen mackerel mention dad writer truck driver night ukiah police try scare transient abandon palace hotel have police dog bark patrol car megaphone bad place aka sunset aka spunset aka skumbset aka perkin street grill hire security post home night say fuk clean bit go town happen travii go place girl brew levi drink sobe zen blend dome get prop girlie sober proper ukiah night prompt little walk stoner park aka park aka mcgarvey park locate street cop shop hang notice west constellation cassiopea giant eastern sky look hear dre eazy e mutha fukin west constellation good thing june brother return europe tuesday look forward day see joe picture bfd totally rule fritch smith fred durst fritch smith pauli shore wacky joe dumb girl upupdowndown end come week unnnhhhh disc work today listen like time shade apart see thing hot water music fuel hate game face face big choice rocket crypt scream dracula scream stick damn stupid head dumb brag stupid music selection fuk spend hour work today break lunch finally project stupid immense project sukke away month life successfully miss ride city fuke ukiah go fun tonight yes work ride city leave minute rule hmmmmm break work past day try ammonia risk management plan work past month thursday pull hour day friday hour day saturday right go absurdity project imagine kind figurative red tape governement put business mention go nerve agent tonight cocodrie co worker brother garrett kristian alto joe phil will feverishly work upupdowndown online day kristian talk girl interrupt realize generally pretty happy delusional hour see movie remember walk ukiah thinking depressed need depressed pretty sweet wake depress day way kinda weird turn round like switch turn follow icq transcript ryan event describe occur weekend pabst pm tell story ukiah good one peteg pm nod party capri smith don germ leather time belong jade merchie smith sober stagger flip chick bump dude like bodhi pabst pm hahahhaha anarcher see puke anarchy sound real story peteg story best peteg pm later wear gear take kevin graham aviator amber shoot glass rock star prompting sententce tilt head nostril ask brush nostril hand pretty funny people take shit real pabst pm hahahahhahahahha niiiiice cool excellent encore peteg pm middle night dave guy wear red drive blue caddy piss smith foot think bed capri loft toilet smith pant pager foot get soak pabst pm bwahahaha smiff kill peteg pm smith get hella defeat dave fuke know go pabst pm teehee story not stop uhn peteg pm friday night go meet crew listen new lc travii walk cougar ask nod beat yr old girl stick head floor window cougar request spit mouth pabst pm cougar ask beat nod peteg pm round corner helluv drunk scarb lean pick dry heaving staggering ask slur random act kindness senseless act pabst pm great anecdote history man peteg pm cougar ball later cougar roll solo sunset see beverage pissing bathroom reach bev leg give nut little squeeze beverage lose elbow face proceed beat shit cougar shiner bruise forehead pabst pm lmao nut little peteg pm early friday night roll low gap nod birthday bbq thing see n cut line coke atm card milwaukee good ice pack box pabst pm god im truly amazed peteg pm talk klebold harris kip kinkel rule love human racist patricia comment imagine think shit get crakke tell pabst pm ahahahha tell people peteg pm start run dry pabst pm big boy peteg pm nod jerry rock girl get ready bar patricia come turn stereo announce time baffle like walk st able understand understand rock get crake pabst pm sweet peteg pm soon start say know person good idea like soon later point know person well know know rokke flip tell fuck tell fun need stop fresh everybody tell crazy pabst pm hahah pete make people crazy sweet peteg pm tell stand guy trippin get punke throw karma peteg pm cougar keep ask think gay night hope work good think gay etc etc ad nauseum pabst pm hahahhaah cougar sexual outlaw peteg pm yeah society bound brown interview yesterday make min wage brewery look shed skrill tell spanky story ironic twist ask guy go pabst pm hahahaha fantastic quote brown peteg pm yeah pretty nice wish get hear stuff second hand instead have live fuk post posterity ukiah daily journal june arrest nathan daugherty ukiah jeremiah williams hopland arrest suspicion disorderly conduct friday block spring street reportedly walk street scream kicking fence apparently jj swinge porch like monkey throw drunk tank night keep play intercom switch way rough jj jj pass johnny law bring breakfast eat meal unnnhhh joy division permanent go cd player work go walmart lunch break see pomo paint van spare tire case return lunch kristian alto office start tentative plan scope nerve agent cocodrie ess eff saturday rumour open explosion jade tree record apparently style punk mosh heavily joe fritch phil bunch work upupdowndown yesterday think end week spanky bail weekend prolly high beer open wound thigh instead go hospital spanky scarb remove staple leg fork leg course get crazy infect susan describe scarb medical smith idea walk bridge night bob virgin take look spanky leg hesitation drive emergency room doctor shock immediately plug anti biotic iv arm tell leg get infected lose release homey tell iv night return morning course spanky take iv able sleep grab molson ice fridge smith break drinking binge ukiah night brandon birthday party pretty good time night pretty fight today helluv wash slip skandal wrong chick realize soon rockin come wolf weekend album key prolly scope like time past day hit face face turn away album heavy nostalgia value prolly go hard certain dissatifaction present eff hear new loose change disc chon friday night helluv good trak hard melodic vein good riddance kid dynamite traditional loose change trak start early new wave kinda inxs feel merge classic loose change form chon duet style vocal track sound cool scope lyric sheet sound pretty good song stuk head weekend right wait drop scope reaction nice band swing thing upupdowndown limp week hope pray smith fritch hit road afi early week go month garrett go alaska week sunday pretty post ukiah pretty excellent kcin say tiger army nerve agent july orange county sound word sure fuke hot uk town night nice night smith walk house mill street ice cream garrett grove walk west key especially avoid skumb hang susan pool forget key ukiah june concert park pretty pack nice change single person go high school ask usual assortment skumb f w go exactly hop entirely suk go new downtown brew pub smith drink ice tea currently bar ban drinking binge uk town comment beer take black bart bar palace cool fairly sucker free zone time dry pay pretty agree game little form wish bfd afi play like shoreline skandalously far away miss lot work lately lifestyle include peep punk o rama tour chico distiller cut tour point sub chico go chico start singe bar room hero murphy set gear point crew lead seriously intense oi chant encore deed dirt ac dc crowd rush stage think place go fall apart chico news union dead guy self release ep record chico state helluv good back chico lot punk year ago cool think visit live go night night soren town everybody pretty festive mood start night new brewery fine go forest hang prop hear exchange state street garrett smoke jay indian chick real indian sober day prop yeah indian crank prop prop want indian smoke weed lace crank speed go bathroom work way bar hear scarb yell hot chick let drunk enter bathroom observe mexicans snort crank toilet later welcome relief intensly shitty night run k rock alto monica julie go julie house mexican work pretty fresh show ticket stub nofx ramone hard rock cafe culican age orange teen idol loose change drie sf aside kristian take jaycon end crocodrie see exactly song loose change set jaycon pretty baffle city pretty interesting talk guy american standard taste punk pretty fucking generic guy pretty mexican band mexico claim play crowd regularly weekend horribly weak drift fever bad dizzy nauseous time fight work bedroom take occasional break deal manage drywall half room know fuk sick like week past month week deal dumb mistake order lp instead cd finally get disc jade tree promise ring feel good helluv good rockin emergency short louder fast proper second release kid dynamite think sound like good riddance understand fuss joan arc memory work pretty good electronic stuff like gimmic know fritch call yesterday thunder bay ontario afi tour go join east coast leg warp tour talk long look forward hear story merchie sell car smiff buy sell ebay today smith go baskin robbin sorbet trad year old skate kid trick pass smith tell able ollie heel kid like know demonstrate high ollie kid tell smith smith attempt heel flip pull attempt walk away kid call smith laugh tradness hop tradillac drive god weekend pretty fucking intense partie cougar night jevon town friday night watch nod ass kick time jevon scarb hear pathetic stop hit come stop hit come nod see jevon drag brandon street scrape face nod destroy tree cougar house shoddy unload gun play talk play russian roullete jevon mag revolver nod get sort fight forest guy claim nod steal ounce weed leverage house guy say state street nod throw state street saturday night trevor host libra party supposse brahma bull get move uk freaks wierdo pop sunset house sober smith pete high red bull wierd sorry sentence lack description prefer relive know post carla yesterday kick frantic cody barge cover dirt like get work yard stare eye guy see eli day call girlfriend bitch go cop tell cody go kill know say kim little piece shit go kim work tell manager harrasse ask go try fire aburptly leave grimm good dog die day chain try jump fence hang half hour later eli show freedom brandon challenge tell fuck store go cause problem tell eli fuck town like cody go bump street maybe talk thing street house skinhead army bedroom window black nike glove sunday night smith garrett walk school street see eli truck hear scuffle bush think eli girl brandy mess smith yell like settle response emphatic fuck smith go grab eli shirt collar headlock brandy drop fucking dog eli face understatement glad eli email send friend see thanksgiving mom say prolly invite friend dinner joe phil interested come family thing fritch get grandparent party black hole smoke lot waste beer spanky nodnarb supply whiskey minute creeper stick head bedroom door person line drop forest club hang college sword town holiday tank ass sing drill sargeant voice spit ill game ill sword chick prison fight wash jock pass black hole sleep blue cigarrette smoke night long wake morning lung splitting gullet tripping head ache walk way town walk shame car let thanksgiving remember time update dog chico pretty wash halloween year time feel like run script hang people house get bar conversation home girl walk shame morning breakfast restruant lousy service good food interested go chico anymore trip chico loose change union dead nod ask hella jealous find boyfriend pete spend night carla blush tell shut good trip word come laura mouth laura yr old sister laura belinda roommate emily carla cd car stereo gangster listen carla car gangster car carla marcy carla mexican homie explain gangster think gangster hang subject oral sex gross guy mark wahlberg pretty fine penis gross gross looking gross feeling feel good hate san diego carla marcy think move diego cool beach music movie ghetto unsolicite long stretch silence lifestyle guess preppie preppie cool wear nice clothe good grade maybe drink little bit problem preppie closet alcoholic preppie unsolicite music like limp roll chico drive frat house bet cute boy discuss desire lomo camera joe mention lubitel come site sell kommie kameras futhermore webmonkey find article produce cool unnh stupid fritch smith show ukiah kick kinda silly smith leather binder thing strap afi carry time case get celly handle business walmart schatts smith get call handle business street walmart cosmetic section look black eye makeup fritch join cell phone posse black posse dude cell phone action purse doormat comsmetic section ukiah walmart yeah conspicuous oh yeah smith mom little brother happen walmart brother came run toy call lie head later go see limit back good movie see preview new sean connery movie say man sum purse cell phone comedy claim number t axe head psycho man chico weekend row time joe come sc town kick deep addition people come chico party time friday night mark rug cutting regular stop weekend lifestyle chico cut floor hard get bill damage thi shit rachael sprain ankle aggravate ankle twist sno boarding week saturday night lar frederiksen bastard play lar know lead singer rancid bastard key play fucking punk style mos def kind thing lar kind punk rock bruce springsteen song topic relate grow campbell lyric good song write set go joe defeat tell look sticker lar pick hand crappy sparkly silver medium stereotype alien head get point lar tight sp modification marshal logo read gear nerve agent go deliver usual brand barely contain chaos remind love see live way listen thi album wrap leave night ahead joe pretty extensive shoot downtown post towne lounge day everybody go way luckily see super bowl matrix style instant replay new budweiser alien waazzzzaaa commercial die happy man end chico weekend jesse jon flipwreck start new band belladonna debut mr lucky saturday night belladonna cool amalgam techno live instrument synthesis claim belladonna fi inch wish kind thing dance particularly spooky helluv back flipwreck good jon jesse continue abe guitar player singer flipwreck good stint hpa south bay move chico plan solo work line elliot smith abe gosh darn talent look forward hear come belladonna set drag queen contest key posse deep gay hick show hick regalia include moustache goatee wrangler hankerchief neck shirt gay cowboy slip finger pocket tight gay cowboy wrangler rub gay cowboy back contest conclude lip sync queen shania twain feel like life wish bring camera tankass think sport wrangler see shit afi recent issue rolling stone magazine pretty baffle brief paragraph long article start way hayfuck town ukiah california come afi ukiah exactly leave spend sunday atop hull mountain learn snowboard mean fall alot twist ankle real bad get home sunday night message mother phil asap need roommate city call wishy washy phil want week kind intense hour hang phil ryan call phil house celly phil lend wait fritch fritch cell phone get press homie ambivelent make lavish change tell think want work home sure think answer guy current job pretty cool claw way somebdoy system lose appeal today get rough start reverse circle block time look suitable parking spot point fuck start head bridge far folsom gear go decide fight downtown traffic double tari neighborhood lap park hill find friday spot buena vista east go hill able roll spot able come go forward hop n bart minute walk work minute late sense sell car owe like good sell prolly shot transmission mean make payment car hit hit car like engine strong forever start get estimate shop figure pay shit dissapointe secretly expect tough plan kind thing car fail bay bridge parking garage welcome chance read hour everyday minute walk work bart pleasant parent move childhood home today wholly form idea actually mean thought like sleep flower bird tree pasture beautiful time puntacte day normal proceeding know event happen today feel like terrosist fly plane building later moment sit financial district coffee shop sip tea pass time little black bird wander street peck crumb litter floor near closing time barista busy work sweep shortage treat little guy ride city decide ac transit bus bay bridge hear good way city bus rarely crowd view ride high bus worth see dark green bus line transbay terminal shiny new like rock roll tour bus window seal suggest air condition luxury see row plush high back forward face seat spend day imagine luxury view relaxtion cruise bay mass transit opulence see come know read website little close avoid right sit bench face perpendicular roadway hot smelly bus kind general route bus see growl hill sf blue trim instead orange trim time bus get move engine roar wind whistle slat window like tea kettle boil fellow commuter case busload teenager raise voice combat racket uncomfortable couch hard plastic vinyl stainless steel bus build hose end day glorious day trip bay area ride rough slide forward time driver goose brake pedal discount brand white high top nice view new bridge hundred time june shocking chill morning fact discover rush hill catch train work fog nestle valley city mist hang air sting face perfect time cite mark twain quip say way san francisco weather naturally train depart station round corner decide walk block coffee shop pass moment cup tea run muni train roll past present location hot tea slop plastic lid burn numbed hand case know muni train bind financial district usually improperly pack people way honest feller drop dime ride feel like citizen pay car bart car usually day work oakland bay bridge san francisco precise work situate exactly border emeryville oakland cinder block business make merchandise entertainment industry fancy way say tshirt band cinder block pretty big client like rem radiohead phil collin stronghold small punk band market little spare time day usually bart train sf oakland work recap past year idea start leave kind interesting exercise put highlight reel recent history eye today performance review work life mean current job month feel like far long good thing indication fit nicely review go boss nail thing generous rate far high rate think push luck official employee access privilege raise secretly hope think people soon tenure raise review dishearten think month obsessively secretly calculate new pay rate good lesson learn school time thing life worth having worth have work appreciate earn usually appreciation proportional work mention work company review procedure give raise steadily work way income ladder think give raise simply good job gmail account car speed work jerk summer hot san francisco ideal payment credit card maxe year hang friend lot start savings account vegetable intake increase talk brothere frequently read book single day sleep drink tea time day eat good restaurant recently people rude idiot aut text mining network dismiss notice tommcamm aut text mining assignment code sign propose sign propose text aut text mining assignment code dismiss notice aut text mining release tommcamm aut text mining assignment code github create cloning archive repository create repository github citation aut text mining text mining assignment code tommcamm aut text cdn worker find file cdn worker find file footer copy footer navigation term privacy security status doc contact manage cookie share personal information not perform action time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Q2cfEiZ9_ZI",
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.912999Z",
          "start_time": "2024-06-01T07:18:08.907563Z"
        }
      },
      "source": [
        "import gzip\n",
        "# Util functions to pre-process the data\n",
        "import pickle\n",
        "\n",
        "def filter_everyone(filepath):\n",
        "    return True\n",
        "\n",
        "def filter_student(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.Student.' in filename\n",
        "\n",
        "def filter_female(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.female.' in filename\n",
        "\n",
        "def filter_male(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    return '.male.' in filename\n",
        "\n",
        "def filter_age_over_20(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        age = int(filename.split('.')[2])\n",
        "        return age > 20\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def filter_age_under_20(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    try:\n",
        "        age = int(filename.split('.')[2])\n",
        "        return age <= 20\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Compress the processed data, otherwise is going to be big.\n",
        "def save_compressed_pickle(data, filename):\n",
        "    with gzip.open(filename, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def load_compressed_pickle(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:18:08.930244Z",
          "start_time": "2024-06-01T07:18:08.914004Z"
        },
        "id": "xq3E0US76kMh"
      },
      "cell_type": "code",
      "source": [
        "# Pre-Processing of the data from smaller to bigger ds\n",
        "# Process \"Students\"\n",
        "#text_data_students, failed_files_students = extract_and_preprocess_text_from_directory(directory_path, filter_student)\n",
        "#save_compressed_pickle(text_data_students, 'students_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"under 20s\"\n",
        "#text_data_under_20s, failed_files_under_20s = extract_and_preprocess_text_from_directory(directory_path, filter_age_under_20)\n",
        "#save_compressed_pickle(text_data_under_20s, 'under_20s_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"males\"\n",
        "#text_data_males, failed_files_males = extract_and_preprocess_text_from_directory(directory_path, filter_male)\n",
        "#save_compressed_pickle(text_data_males, 'males_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"females\"\n",
        "#text_data_females, failed_files_females = extract_and_preprocess_text_from_directory(directory_path, filter_female)\n",
        "#save_compressed_pickle(text_data_females, 'females_preprocessed.pkl.gz')\n",
        "\n",
        "# Process \"Over 20s\" - TODO\n",
        "#text_data_over_20s, failed_files_over_20s = extract_and_preprocess_text_from_directory(directory_path, filter_age_over_20)\n",
        "#save_compressed_pickle(text_data_over_20s, 'over_20s_preprocessed.pkl.gz')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:23:46.616176Z",
          "start_time": "2024-06-01T07:23:41.952413Z"
        },
        "id": "Be_-Dhjr6kMh"
      },
      "cell_type": "code",
      "source": [
        "# Load of all pre-processed files\n",
        "students_data = load_compressed_pickle('PreProcessed/students_preprocessed.pkl.gz')\n",
        "#under_20s_data = load_compressed_pickle('under_20s_preprocessed.pkl.gz')\n",
        "#males_data = load_compressed_pickle('')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SDU3M6UI6kMi"
      },
      "cell_type": "markdown",
      "source": [
        "## Topic modeling with LDA\n",
        "\n",
        "1. With students"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T07:46:51.480526Z",
          "start_time": "2024-06-01T07:46:10.434531Z"
        },
        "id": "NrJNMIUJ6kMi",
        "outputId": "70cf1963-f0d2-4604-d8b5-55cbccd1677e"
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Topic modeling with LDA - Students\n",
        "students_tokens = [get_tokens_without_pos(doc) for doc in students_data]\n",
        "\n",
        "# Dictionary representation of all docs\n",
        "id2word = corpora.Dictionary(students_tokens)\n",
        "\n",
        "# Corpus representation of all docs\n",
        "corpus = [id2word.doc2bow(text) for text in students_tokens]\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=2,\n",
        "    random_state=100,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "print(lda_model.print_topics())"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.015*\"like\" + 0.014*\"go\" + 0.010*\"love\" + 0.010*\"know\" + 0.009*\"get\" + 0.009*\"think\" + 0.008*\"good\" + 0.007*\"time\" + 0.007*\"thing\" + 0.007*\"not\"'), (1, '0.011*\"like\" + 0.009*\"go\" + 0.007*\"think\" + 0.007*\"time\" + 0.007*\"know\" + 0.007*\"get\" + 0.007*\"good\" + 0.006*\"day\" + 0.006*\"people\" + 0.006*\"thing\"')]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-01T08:06:49.721724Z",
          "start_time": "2024-06-01T07:53:33.401164Z"
        },
        "id": "VOHDSeEG6kMi",
        "outputId": "0016a508-8bfe-4869-d54b-62c7d166338b"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Increase the max_length limit of SpaCy\n",
        "nlp.max_length = 1500000\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, max_chunk_size=1000000):\n",
        "    return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "# Extract tokens from pre-processed students_data\n",
        "def extract_tokens(documents):\n",
        "    return [[token for token, _ in doc] for doc in documents]\n",
        "\n",
        "documents = extract_tokens(students_data)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "id2word = corpora.Dictionary(documents)\n",
        "corpus = [id2word.doc2bow(text) for text in documents]\n",
        "\n",
        "# Build the LDA model using multicore processing\n",
        "lda_model = gensim.models.LdaMulticore(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=2,\n",
        "    random_state=100,\n",
        "    chunksize=100,\n",
        "    passes=10,\n",
        "    per_word_topics=True\n",
        ")\n",
        "\n",
        "# Get the top terms for each topic\n",
        "top_terms = lda_model.show_topics(num_words=5)\n",
        "topics = [term for topic in top_terms for term, _ in lda_model.show_topic(topic[0], topn=5)]\n",
        "print(\"Top Terms:\", topics)\n",
        "\n",
        "def extract_sentences_with_topics(text, topics):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents if any(topic in sent.text for topic in topics)]\n",
        "    return sentences\n",
        "\n",
        "# Combine all documents into a single text for sentence extraction\n",
        "combined_text = \" \".join([\" \".join(doc) for doc in documents])\n",
        "\n",
        "# Split the combined text into smaller chunks\n",
        "text_chunks = split_text_into_chunks(combined_text)\n",
        "\n",
        "# Extract sentences with topics from each chunk\n",
        "all_sentences_with_topics = []\n",
        "for chunk in text_chunks:\n",
        "    sentences_with_topics = extract_sentences_with_topics(chunk, topics)\n",
        "    all_sentences_with_topics.extend(sentences_with_topics)\n",
        "\n",
        "print(\"Total sentences with topics:\", len(all_sentences_with_topics))\n",
        "for sentence in all_sentences_with_topics:\n",
        "    print(sentence)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Terms: ['like', 'go', 'love', 'know', 'get', 'like', 'go', 'think', 'time', 'get']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}