{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Session 2 demostration code**\n",
    "**Tokenization and n-gram extraction.**\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Gi-sIaTLA4Hw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "id": "0FZjqhqnB24R",
    "outputId": "629cc7d1-e7a1-4959-f1e8-8e411bd1e0b4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T07:33:52.419934Z",
     "start_time": "2024-03-10T07:33:50.410320Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tommc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BkB053s_0ZoN",
    "outputId": "5779b826-3ece-4652-db6b-71c98620ecd0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T07:34:05.319998Z",
     "start_time": "2024-03-10T07:34:05.300915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'city', 'of', 'Auckland', 'is', 'in', 'New', 'Zealand', 'which', 'is', 'in', 'the', 'Pacific']\n",
      "['It', 'does', \"n't\", 'matter', 'what', 'the', 'name', 'is', '.']\n",
      "['The', 'car', \"'s\", 'tyres', 'were', 'big', '.']\n",
      "['This', 'is', 'an', 'exmaple', 'of', 'pre-processing']\n"
     ]
    }
   ],
   "source": [
    "######################Simple Tagging###############################################################################################\n",
    "text1 = \"The city of Auckland is in New Zealand which is in the Pacific\"\n",
    "text2 = \"It doesn't matter what the name is.\"\n",
    "text3 = \"The car's tyres were big.\"\n",
    "text4 = \"This is an exmaple of pre-processing\"\n",
    "\n",
    "\n",
    "text = nltk.word_tokenize(text1)\n",
    "print(text)\n",
    "text = nltk.word_tokenize(text2)\n",
    "print(text)\n",
    "text = nltk.word_tokenize(text3)\n",
    "print(text)\n",
    "text = nltk.word_tokenize(text4)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use of a different tokenizer\n",
    "Observe the difference!"
   ],
   "metadata": {
    "id": "uFxKkqWnCHEx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.utils import tokenize # Note: this library is not part of nltk so install using \"pip3 install gensim\" if required.\n",
    "print(list(tokenize(text1)))\n",
    "print(list(tokenize(text2)))\n",
    "print(list(tokenize(text3)))\n",
    "print(list(tokenize(text4)))"
   ],
   "metadata": {
    "id": "7bLGjSgsCRhe",
    "outputId": "bc4fc6f7-c90c-4d23-aaf0-a2ecf60a8817",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:14.453676Z",
     "start_time": "2024-03-10T07:38:13.683974Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'city', 'of', 'Auckland', 'is', 'in', 'New', 'Zealand', 'which', 'is', 'in', 'the', 'Pacific']\n",
      "['It', 'doesn', 't', 'matter', 'what', 'the', 'name', 'is']\n",
      "['The', 'car', 's', 'tyres', 'were', 'big']\n",
      "['This', 'is', 'an', 'exmaple', 'of', 'pre', 'processing']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing Examples\n",
    "Cleaning strings"
   ],
   "metadata": {
    "id": "oqApSbghNd7m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove numbers"
   ],
   "metadata": {
    "id": "-OHGB0oWNzPc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "text5 = 'There are 6 cities in New Zealand and 2 of which are in the North Island'\n",
    "result = re.sub(r'\\d+', '', text5)\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diGogstLNewX",
    "outputId": "68dcd9e5-6619-4554-d12f-20178988e831",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:18.736633Z",
     "start_time": "2024-03-10T07:38:18.732190Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  cities in New Zealand and  of which are in the North Island\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove punctuation- member of [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
   ],
   "metadata": {
    "id": "Of7KwzVrOoK_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "text6 = 'Have you had your lunch? If not Peter/s (He lives nearby) father can get you some!'\n",
    "result = text6.translate(str.maketrans(' ',' ', string.punctuation))\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuqojgZpOoyc",
    "outputId": "87fecd77-ebb2-46f3-cd51-2270450faa34",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:22.554404Z",
     "start_time": "2024-03-10T07:38:22.548996Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you had your lunch If not Peters He lives nearby father can get you some\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##remove white spaces"
   ],
   "metadata": {
    "id": "8jIb8o_SROrd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text7 = ' \\t a string example with tabs\\t'\n",
    "print(text7)\n",
    "text7 = text7.strip()\n",
    "print(text7)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYqnT4mSRSav",
    "outputId": "c453b32b-500d-495b-81a8-bea249477716",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:25.549215Z",
     "start_time": "2024-03-10T07:38:25.542749Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t a string example with tabs\t\n",
      "a string example with tabs\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove stop words"
   ],
   "metadata": {
    "id": "5XUXrENDSTZU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##NLTK"
   ],
   "metadata": {
    "id": "EW1k1URgY1DP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ONG8qlbU6UH",
    "outputId": "c4984d86-d998-4470-e889-25dbaaec9f98",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:31.329100Z",
     "start_time": "2024-03-10T07:38:30.881906Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tommc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_NLTK = set(stopwords.words('english'))\n",
    "print(stop_words_NLTK)\n",
    "tokens = tokenize(text1)\n",
    "result = [i for i in tokens if not i in stop_words_NLTK]\n",
    "print (result)\n",
    "my_stop_words = stop_words_NLTK\n",
    "print(len(my_stop_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhUvb032SUAO",
    "outputId": "4c3de221-f8cd-466d-f00b-ba7d0f21d073",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:36.701151Z",
     "start_time": "2024-03-10T07:38:36.685650Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"should've\", \"she's\", 'her', 'again', 'such', 'same', 'the', 'once', 'after', 'ma', 'those', 'were', 'been', 'few', \"shouldn't\", \"haven't\", 'when', 'yours', 'i', 'he', 'off', 'down', 'and', 'they', \"won't\", 'your', 'below', 'have', 'about', 'm', 'than', 'hasn', \"couldn't\", 'does', 'd', 'theirs', 'myself', 'further', \"shan't\", 'do', 'him', \"didn't\", 'them', 'can', \"wasn't\", 'no', \"wouldn't\", 'for', 'ourselves', 'hadn', 'be', 'a', 'my', 'with', 'herself', 'during', 'too', 'his', 's', 'into', 'while', 'any', 'out', 'as', 't', 'just', \"hasn't\", 'don', 'our', 'by', 'most', 'because', 'shouldn', 'against', 'before', 'an', 'not', 'who', 'of', 'mightn', \"isn't\", 'did', \"you'll\", 'why', 'o', 'until', \"you'd\", 'some', 'doing', 'own', 'themselves', 'yourself', 'how', 'to', 'which', 'didn', 'needn', 'you', 'now', 'itself', 'up', 'above', 'where', 'had', 'so', \"don't\", 'at', 'here', 'it', \"hadn't\", 'very', 'if', \"doesn't\", 'shan', 'wasn', 're', 'being', 'their', 'other', 'y', 'hers', 'or', 'on', \"aren't\", 'from', 'ours', 'through', 'these', 'won', 'there', \"needn't\", \"mustn't\", \"mightn't\", 'both', 'more', 'but', 'she', 'me', 'ain', 'that', 'wouldn', 'all', 'am', 'over', 'what', 'then', 'yourselves', 'should', 'each', 'are', 've', \"it's\", 'aren', 'isn', 'will', 'couldn', 'doesn', 'weren', 'has', \"you've\", 'its', 'll', 'between', 'whom', 'only', 'haven', 'is', 'in', 'we', 'nor', \"you're\", 'having', 'mustn', 'this', 'under', \"weren't\", 'was', \"that'll\", 'himself'}\n",
      "['The', 'city', 'Auckland', 'New', 'Zealand', 'Pacific']\n",
      "179\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other sets of stop words\n",
    "## sklearn"
   ],
   "metadata": {
    "id": "PIQbjrdkY4Jv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words_sklearn = text.ENGLISH_STOP_WORDS\n",
    "print(stop_words_sklearn)\n",
    "my_stop_words = stop_words_sklearn.union(my_stop_words)\n",
    "print(len(my_stop_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0S91CIkHV8Bv",
    "outputId": "771bcd8b-4a0a-488b-e193-ac9d281ebdcd",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:38:55.859774Z",
     "start_time": "2024-03-10T07:38:55.714739Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'also', 'whereby', 'always', 'were', 'sometime', 'off', 'moreover', 'and', 'noone', 'almost', 'five', 'twelve', 'than', 'elsewhere', 'seem', 'empty', 'even', 'myself', 'can', 'everyone', 'no', 'latter', 'several', 'ourselves', 'towards', 'show', 'during', 'with', 'interest', 'everything', 'around', 'side', 'thick', 'because', 'together', 'hasnt', 'indeed', 'anyway', 'becomes', 'already', 'own', 'yourself', 'move', 'often', 'how', 'to', 'so', 'next', 'us', 'less', 'very', 'if', 'latterly', 'somewhere', 'their', 'other', 'on', 'from', 'mine', 'full', 'there', 'since', 'much', 'both', 'more', 'alone', 'me', 'am', 'upon', 'eg', 'then', 'though', 'yourselves', 'eight', 'mostly', 'least', 'hereafter', 'rather', 'is', 'nine', 'nobody', 'name', 'this', 'put', 'may', 'seems', 'cant', 'her', 'again', 'such', 'ever', 'same', 'those', 'therein', 'done', 'whoever', 'when', 'yours', 'becoming', 'he', 'ltd', 'describe', 'therefore', 'thin', 'have', 'inc', 'cry', 'about', 'whenever', 'get', 'a', 'although', 'my', 'any', 'beyond', 'as', 'part', 'whence', 'ten', 'fire', 'of', 'eleven', 'whole', 'why', 'never', 'themselves', 'up', 'toward', 'un', 'above', 'thus', 'had', 'here', 'whither', 'two', 'could', 'however', 'thence', 'ours', 'beside', 'first', 'con', 'she', 'well', 'whereafter', 'all', 'what', 'nothing', 'take', 'none', 'will', 'its', 'between', 'only', 'became', 'in', 'last', 'sixty', 'anything', 'everywhere', 'bottom', 'besides', 'detail', 'co', 'another', 'either', 'nowhere', 'i', 'they', 'whatever', 'your', 'top', 'below', 'via', 'made', 'etc', 'further', 'do', 'him', 'them', 'fifty', 'bill', 'be', 'neither', 'herself', 'thereupon', 'his', 'while', 'give', 'our', 'by', 'before', 'hereby', 'an', 'six', 'not', 'beforehand', 'behind', 'who', 'someone', 'forty', 'until', 'yet', 'some', 'become', 'thereafter', 'which', 'you', 'call', 'where', 'third', 'within', 'thereby', 'it', 'system', 'every', 're', 'being', 'or', 'through', 'these', 'wherein', 'per', 'across', 'sincere', 'hundred', 'keep', 'that', 'whose', 'fifteen', 'has', 'found', 'others', 'due', 'see', 'amount', 'hence', 'nor', 'under', 'was', 'perhaps', 'ie', 'once', 'the', 'after', 'seemed', 'whereas', 'few', 'been', 'among', 'anywhere', 'down', 'herein', 'along', 'might', 'whether', 'except', 'please', 'thru', 'must', 'for', 'many', 'too', 'back', 'nevertheless', 'would', 'into', 'out', 'something', 'somehow', 'most', 'namely', 'against', 'fill', 'couldnt', 'without', 'now', 'itself', 'front', 'three', 'at', 'anyone', 'de', 'find', 'otherwise', 'hers', 'sometimes', 'twenty', 'anyhow', 'still', 'seeming', 'hereupon', 'whereupon', 'wherever', 'amongst', 'amoungst', 'but', 'formerly', 'over', 'should', 'four', 'former', 'each', 'are', 'else', 'onto', 'go', 'afterwards', 'serious', 'throughout', 'whom', 'meanwhile', 'we', 'enough', 'cannot', 'one', 'mill', 'himself'})\n",
      "378\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spacy"
   ],
   "metadata": {
    "id": "Hg4OAfB3Y70m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words_Spacy = STOP_WORDS\n",
    "print(stop_words_Spacy)\n",
    "my_stop_words = stop_words_Spacy.union(my_stop_words)\n",
    "print(my_stop_words)\n",
    "print(len(my_stop_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kipoJH_Y9wG",
    "outputId": "a8181e28-58e7-4508-a4e2-a84c5e24dfea",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:40:52.902275Z",
     "start_time": "2024-03-10T07:40:52.289094Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'also', 'whereby', 'always', 'were', 'sometime', 'off', 'moreover', 'and', 'noone', 'almost', 'five', 'twelve', 'than', 'elsewhere', 'seem', 'empty', 'even', 'myself', 'can', 'everyone', 'no', 'latter', 'several', 'ourselves', 'towards', 'show', 'during', 'with', 'everything', 'around', 'side', 'just', 'because', 'together', 'quite', 'various', 'indeed', 'anyway', 'becomes', 'already', 'own', 'yourself', 'move', 'often', 'how', 'to', 'so', 'next', 'us', 'less', 'very', 'if', 'latterly', 'somewhere', 'their', 'other', 'on', 'from', 'mine', 'full', 'there', 'since', 'much', 'both', 'more', 'alone', 'me', 'am', 'upon', 'then', 'though', 'yourselves', 'eight', 'mostly', 'least', 'hereafter', 'rather', 'is', 'nine', 'nobody', \"'ve\", 'name', 'this', 'put', 'seems', 'may', 'her', 'again', 'such', 'ever', 'same', 'those', 'therein', 'done', 'whoever', '‘ll', 'when', 'yours', 'becoming', 'he', 'therefore', '‘re', 'have', 'about', '’d', 'whenever', 'get', 'a', 'although', 'my', 'any', 'beyond', 'as', 'part', 'whence', '’m', 'ten', 'of', 'eleven', 'whole', 'unless', 'why', 'never', 'themselves', '‘d', 'up', 'toward', \"'re\", 'above', 'thus', 'had', 'here', 'whither', 'two', 'could', 'however', 'thence', 'ours', \"'m\", '’ll', 'beside', 'first', 'she', 'well', 'whereafter', 'all', \"n't\", 'what', 'nothing', '’re', 'take', 'none', 'will', 'its', 'say', 'between', 'only', 'became', 'in', 'last', 'sixty', 'anything', 'ca', 'everywhere', '‘ve', 'bottom', 'besides', 'another', '’s', 'either', 'nowhere', 'i', '‘m', 'they', 'whatever', 'your', 'top', 'below', 'via', 'made', 'further', 'do', 'him', 'them', 'fifty', 'be', 'neither', 'herself', 'thereupon', 'his', 'while', 'give', 'our', 'by', 'before', 'hereby', 'an', 'six', 'not', 'beforehand', 'behind', 'who', 'using', 'someone', 'forty', 'until', 'yet', 'some', 'become', 'thereafter', 'which', 'you', 'call', 'where', 'third', 'within', 'thereby', 'it', 'every', 're', 'being', 'or', 'n’t', 'through', 'these', 'wherein', 'per', 'across', 'hundred', 'keep', 'that', 'whose', 'n‘t', 'really', 'fifteen', 'has', 'regarding', 'others', 'due', 'see', 'amount', 'hence', 'nor', 'under', 'was', 'make', 'perhaps', '‘s', 'once', 'the', 'after', 'seemed', 'whereas', 'few', 'been', 'among', 'anywhere', 'down', 'herein', 'along', 'might', 'whether', 'except', 'please', 'thru', 'does', 'must', 'for', 'many', 'too', 'back', 'nevertheless', 'would', 'into', 'used', 'out', 'something', 'somehow', 'most', 'namely', 'against', 'did', \"'s\", 'doing', 'without', 'now', 'itself', 'front', 'three', 'at', 'anyone', '’ve', 'otherwise', 'hers', 'sometimes', 'twenty', 'anyhow', 'still', 'seeming', 'hereupon', 'whereupon', 'wherever', 'amongst', 'but', 'formerly', 'over', 'should', 'four', 'former', 'each', 'are', 'else', 'onto', 'go', 'afterwards', 'serious', 'throughout', 'whom', 'meanwhile', \"'ll\", 'we', 'enough', 'cannot', \"'d\", 'one', 'himself'}\n",
      "{'also', 'whereby', 'always', 'were', 'sometime', 'off', 'moreover', 'and', 'noone', 'almost', 'five', 'twelve', 'm', 'than', 'hasn', 'elsewhere', 'seem', 'empty', 'even', 'myself', \"shan't\", 'can', \"wasn't\", 'everyone', 'no', 'latter', 'several', \"wouldn't\", 'ourselves', 'towards', 'show', 'during', 'with', 'interest', 'everything', 'around', 'side', 'thick', 'just', 'because', 'together', 'quite', 'various', 'indeed', 'hasnt', 'mightn', 'anyway', 'becomes', 'already', 'own', 'yourself', 'move', 'often', 'how', 'to', 'didn', 'needn', 'so', 'next', 'us', 'less', \"hadn't\", 'very', 'if', 'latterly', 'wasn', 'somewhere', 'their', 'other', 'on', 'from', 'mine', 'full', 'there', 'since', 'much', \"mightn't\", 'both', 'more', 'alone', 'me', 'am', 'upon', 'eg', 'then', 'though', 'yourselves', 'eight', 'mostly', 'isn', 'least', 'hereafter', 'rather', 'is', 'nine', 'nobody', \"'ve\", 'name', 'this', 'put', 'seems', 'may', 'cant', 'her', 'again', 'such', 'ever', 'same', 'those', 'therein', 'done', \"shouldn't\", 'whoever', \"haven't\", '‘ll', 'when', 'yours', 'becoming', 'he', 'ltd', 'describe', 'therefore', '‘re', 'thin', 'have', 'inc', 'cry', 'about', '’d', 'whenever', 'get', 'd', 'hadn', 'a', 'although', 'my', 'any', 'beyond', 'as', 'part', 'whence', '’m', \"hasn't\", 'don', 'ten', 'fire', 'of', 'eleven', 'whole', 'unless', 'why', 'never', 'themselves', '‘d', 'up', 'toward', \"'re\", 'un', 'above', 'thus', 'had', 'here', 'whither', 'two', 'could', 'however', 'thence', 'ours', \"'m\", '’ll', 'beside', 'first', \"needn't\", 'con', \"mustn't\", 'she', 'well', 'whereafter', 'all', \"n't\", 'what', 'nothing', '’re', 'take', 'none', 'will', 'couldn', 'its', \"you've\", 'say', 'between', 'only', 'became', 'in', 'last', 'sixty', \"you're\", 'anything', 'ca', \"should've\", 'everywhere', '‘ve', 'bottom', 'besides', 'detail', 'co', 'another', '’s', 'either', 'nowhere', 'i', '‘m', 'they', 'whatever', 'your', 'top', 'below', 'via', 'made', \"couldn't\", 'etc', 'further', 'do', 'him', \"didn't\", 'them', 'fifty', 'bill', 'be', 'neither', 'herself', 'thereupon', 'his', 'while', 'give', 't', 'our', 'by', 'shouldn', 'before', 'hereby', 'an', 'six', 'not', 'beforehand', 'behind', 'who', 'using', 'someone', 'forty', 'o', 'until', 'yet', 'some', \"you'd\", 'become', 'thereafter', 'which', 'you', 'call', 'where', \"don't\", 'third', 'within', 'thereby', 'it', 'system', 'every', 're', 'being', 'or', 'n’t', 'through', 'these', 'wherein', 'won', 'per', 'across', 'sincere', 'hundred', 'keep', 'ain', 'that', 'whose', 've', 'n‘t', 'really', \"it's\", 'doesn', 'weren', 'fifteen', 'has', 'found', 'll', 'regarding', 'others', 'due', 'haven', 'see', 'amount', 'hence', 'nor', 'mustn', 'under', \"weren't\", 'was', \"that'll\", 'make', \"she's\", 'perhaps', '‘s', 'ie', 'once', 'the', 'after', 'ma', 'seemed', 'whereas', 'few', 'been', 'among', 'anywhere', 'down', \"won't\", 'herein', 'along', 'might', 'whether', 'except', 'please', 'thru', 'does', 'theirs', 'must', 'for', 'many', 'too', 'back', 'nevertheless', 'would', 's', 'into', 'used', 'out', 'something', 'somehow', 'most', 'namely', 'against', 'fill', \"isn't\", 'did', \"you'll\", \"'s\", 'doing', 'couldnt', 'without', 'now', 'itself', 'front', 'three', 'at', 'anyone', 'de', '’ve', 'find', \"doesn't\", 'shan', 'otherwise', 'y', 'hers', 'sometimes', \"aren't\", 'twenty', 'anyhow', 'still', 'seeming', 'hereupon', 'whereupon', 'wherever', 'amongst', 'amoungst', 'but', 'formerly', 'wouldn', 'over', 'should', 'four', 'former', 'each', 'are', 'else', 'onto', 'aren', 'go', 'afterwards', 'serious', 'throughout', 'whom', 'meanwhile', \"'ll\", 'we', 'enough', 'having', 'cannot', \"'d\", 'one', 'mill', 'himself'}\n",
      "409\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stemming"
   ],
   "metadata": {
    "id": "zKZAxB_VcGSf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "text10 = 'The city of Auckalnd\\'s team is Blues but its not confirmed'\n",
    "print(text1)\n",
    "text10_tokens=word_tokenize(text10)\n",
    "for word in text10_tokens:\n",
    "    print(stemmer.stem(word))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJb8saiicGws",
    "outputId": "99fde03d-4e54-4372-f6d7-88fdbfc48628",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:41:00.733140Z",
     "start_time": "2024-03-10T07:41:00.727862Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city of Auckland is in New Zealand which is in the Pacific\n",
      "the\n",
      "citi\n",
      "of\n",
      "auckalnd\n",
      "'s\n",
      "team\n",
      "is\n",
      "blue\n",
      "but\n",
      "it\n",
      "not\n",
      "confirm\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatization"
   ],
   "metadata": {
    "id": "yaLwZVcOcpu8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLLXFHVpdQ8S",
    "outputId": "f0bf2615-3536-46f5-85c4-6235e81f11fc",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:41:06.604560Z",
     "start_time": "2024-03-10T07:41:02.998349Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tommc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tommc\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for word in text10_tokens:\n",
    "    print(lemmatizer.lemmatize(word))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoCIqqQLctRY",
    "outputId": "3fd096d6-16fd-41c9-fe8e-9e184242b815",
    "ExecuteTime": {
     "end_time": "2024-03-10T07:41:09.895836Z",
     "start_time": "2024-03-10T07:41:08.864212Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "city\n",
      "of\n",
      "Auckalnd\n",
      "'s\n",
      "team\n",
      "is\n",
      "Blues\n",
      "but\n",
      "it\n",
      "not\n",
      "confirmed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lets write a function to extract bi-grams\n",
    "Also observe how we can extract the components of the bi-gram. Can you modify the funciton to extract tri-grams?\n"
   ],
   "metadata": {
    "id": "mYaUG5BiDP7Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import  nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"The quick brown fox jumped on the lazy dog.\")\n",
    "def find_bigrams(input_list):\n",
    "  bigram_list = []\n",
    "  for i in range(len(input_list)-1):\n",
    "      bigram_list.append((input_list[i], input_list[i+1]))\n",
    "\n",
    "  return bigram_list\n",
    "#get individual items from the bigram\n",
    "bigrams = find_bigrams(text)\n",
    "print(bigrams)\n",
    "print(len(bigrams))\n",
    "print(bigrams[2].__getitem__(0))\n",
    "print(bigrams[2].__getitem__(1))"
   ],
   "metadata": {
    "id": "6CXo7daEDfUQ",
    "outputId": "99f72e86-d572-4c33-beb6-58ac3a0e3ac2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T07:41:14.103360Z",
     "start_time": "2024-03-10T07:41:14.096963Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumped'), ('jumped', 'on'), ('on', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', '.')]\n",
      "9\n",
      "brown\n",
      "fox\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lets use the n-gram function from NLTK"
   ],
   "metadata": {
    "id": "7OQ74rwtEoEt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'The quick brown fox jumped over the lazy dog.'\n",
    "n = 3\n",
    "gramsList = ngrams(sentence.split(), n)\n",
    "ngrams = []\n",
    "for grams in gramsList:\n",
    "  ngrams.append(grams)\n",
    "print(ngrams)"
   ],
   "metadata": {
    "id": "mH2PJi-VE0SH",
    "outputId": "15f628c7-dd7e-4c60-9662-cb812777363b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-03-10T07:41:32.214330Z",
     "start_time": "2024-03-10T07:41:32.209707Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumped'), ('fox', 'jumped', 'over'), ('jumped', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog.')]\n"
     ]
    }
   ]
  }
 ]
}
